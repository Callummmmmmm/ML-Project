{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b2041111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_imports import *\n",
    "from data_clean import data\n",
    "clean_df = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HGBR_shap(input_data, var, top_n):\n",
    "    top_n = top_n - 1\n",
    "    # Seperating the dataframes based on missing O3 values and assinging to variables.\n",
    "    df_missing = input_data[input_data[var].isnull()]\n",
    "    df_not_missing = input_data.dropna(subset = [var])\n",
    "\n",
    "    # Seperating features and target variable.\n",
    "    X = df_not_missing.drop(columns = [var])\n",
    "    y = df_not_missing[var]\n",
    "    \n",
    "    # Splitting the data into train and testing subsets.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "    \n",
    "    # Training the model.\n",
    "    clf = HistGradientBoostingRegressor(random_state = 1)    \n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    explainer = shap.Explainer(clf)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "    top_n_indices = np.argsort(shap_importance)[-top_n:][::-1]  # Indices of the top 10 features\n",
    "    top_n_features = X.columns[top_n_indices]\n",
    "    return input_data[top_n_features.to_list() + [var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "64e3abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HGBR(input_data, var, shap_i, state):\n",
    "    # Seperating the dataframes based on missing O3 values and assinging to variables.\n",
    "    df_missing = input_data[input_data[var].isnull()]\n",
    "    df_not_missing = input_data.dropna(subset = [var])\n",
    "\n",
    "    # Seperating features and target variable.\n",
    "    X = df_not_missing.drop(columns = [var])\n",
    "    y = df_not_missing[var]\n",
    "    \n",
    "    # Splitting the data into train and testing subsets.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "    \n",
    "    # Training the model.\n",
    "    clf = HistGradientBoostingRegressor(random_state = 1)    \n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on the training set.\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    # Predictions on the test set.\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    \n",
    "    if shap_i == True:\n",
    "        explainer = shap.Explainer(clf)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap.summary_plot(shap_values, X_test)\n",
    "    \n",
    "    # Calculating regression metrics for the training set.\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_train = mse_train ** 0.5\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    # Calculating regression metrics for the test set.\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = mse_test ** 0.5\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    if state == True:\n",
    "        # Print training set regression metrics.\n",
    "        print(f'Training Set - MSE: {mse_train:8.5f}, RMSE: {rmse_train:8.5f}, MAE: {mae_train:8.5f}, R2: {r2_train:8.5f}')\n",
    "\n",
    "        # Print test set regression metrics.\n",
    "        print(f'Testing Set  - MSE: {mse_test:8.5f}, RMSE: {rmse_test:8.5f}, MAE: {mae_test:8.5f}, R2: {r2_test:8.5f}')\n",
    "    else:\n",
    "        return [[mse_train, rmse_train, mae_train, r2_train], [mse_test, rmse_test, mae_test, r2_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5b0ffeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance(features):\n",
    "\n",
    "    feature_range = range(1, features)\n",
    "\n",
    "    MSE = [[], []]\n",
    "    RMSE = [[], []]\n",
    "    MAE = [[], []]\n",
    "    R2 = [[], []]\n",
    "\n",
    "    metrics = [MSE, RMSE, MAE, R2]\n",
    "    metric_names = ['MSE', 'RMSE', 'MAE', 'R2']\n",
    "\n",
    "    for n in feature_range:\n",
    "\n",
    "        shap_var_df = HGBR_shap(clean_df, 'O3_ppbV', n)\n",
    "        HGBR_m = HGBR(shap_var_df, 'O3_ppbV', False, False)\n",
    "        MSE[0].append(HGBR_m[0][0])\n",
    "        MSE[1].append(HGBR_m[1][0])\n",
    "        RMSE[0].append(HGBR_m[0][1])\n",
    "        RMSE[1].append(HGBR_m[1][1])\n",
    "        MAE[0].append(HGBR_m[0][2])\n",
    "        MAE[1].append(HGBR_m[1][2])\n",
    "        R2[0].append(HGBR_m[0][3])\n",
    "        R2[1].append(HGBR_m[1][3])\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        axs[i].plot(feature_range, metric[0], marker='o', label=f'Training {metric_names[i]}')\n",
    "        axs[i].plot(feature_range, metric[1], marker='o', label=f'Testing {metric_names[i]}')\n",
    "        axs[i].set_title(f'{metric_names[i]} Over Different Configurations')\n",
    "        axs[i].set_xlabel('Configuration Number')\n",
    "        axs[i].set_ylabel(metric_names[i])\n",
    "        axs[i].legend()\n",
    "        axs[i].grid(True)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series(input_data, var, top_n):\n",
    "    imp_var_df = HGBR_shap(input_data, var, top_n)\n",
    "    exc_var_df = imp_var_df.drop(columns = [var])\n",
    "    for variable in exc_var_df:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(imp_var_df[var].values, linestyle='-', color='r', linewidth = 0.1)\n",
    "        plt.plot(exc_var_df[variable].values, linestyle='-', color='b', linewidth = 0.1)\n",
    "        plt.title(f'Line Graph of {variable} and {var}')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(variable)\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
